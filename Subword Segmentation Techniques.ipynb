{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare BPE, SentencePiece, and Bert (pretrained) tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before that, let's look at a word tokenizer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', \"'s\", 'a', 'son-in-law', ',', 'mother-in-law', ',', 'etc', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"There's a son-in-law, mother-in-law, etc.\"\n",
    "tokens = word_tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get the original sentence from the tokens?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There 's a son-in-law , mother-in-law , etc .\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There'sason-in-law,mother-in-law,etc.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tricky!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rico Sennrich, Barry Haddow and Alexandra Birch (2016): [Neural Machine Translation of Rare Words with Subword Units](http://www.aclweb.org/anthology/P16-1162) Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n",
    "\n",
    "\"Byte Pair Encoding (BPE) (Gage, 1994) is a simple data compression technique that iteratively replaces  the  most  frequent  pair  of  bytes  in  a  sequence with a single, unused byte.  We adapt this algorithm for word segmentation. Instead of merging frequent pairs of bytes, we merge characters or\n",
    "character sequence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/rsennrich/subword-nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"pip install subword-nmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play a little with a toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "low\n",
      "low\n",
      "low\n",
      "low\n",
      "lower\n",
      "lower\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "widest\n",
      "widest\n",
      "widest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's create a sample text\n",
    "# This is the same example as the one in the above paper.\n",
    "text =\"low\\n\"*5 + \"lower\\n\"*2 + \"newest\\n\"*6 + \"widest\\n\"*3\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('toy', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1. Learn bpe.  \n",
    "Process byte pair encoding and generate merge operations, i.e., codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that -s means number of operations\n",
    "learn_bpe = \"subword-nmt learn-bpe -s 1 --min-frequency 2 < toy > codes\"\n",
    "os.system(learn_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==codes==\n",
      "#version: 0.2\n",
      "s t</w>\n",
      "====\n",
      "number of codes:  1\n"
     ]
    }
   ],
   "source": [
    "codes = open('codes', 'r').read()\n",
    "print(\"==codes==\\n\" + codes + \"====\")\n",
    "print(\"number of codes: \", len(codes.splitlines())-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "△ `</w>` means end of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "△ Check the toy sample carefully. The last 9 words end in `st`, which is most frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2. Apply bpe.    \n",
    "Apply codes to the designated file such that the original text is segmented.\n",
    "For demo, we apply the codes to the same toy file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_bpe = \"subword-nmt apply-bpe -c codes < toy > bpe\"\n",
    "os.system(apply_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==segmented==\n",
      "l@@ o@@ w\n",
      "l@@ o@@ w\n",
      "l@@ o@@ w\n",
      "l@@ o@@ w\n",
      "l@@ o@@ w\n",
      "l@@ o@@ w@@ e@@ r\n",
      "l@@ o@@ w@@ e@@ r\n",
      "n@@ e@@ w@@ e@@ st\n",
      "n@@ e@@ w@@ e@@ st\n",
      "n@@ e@@ w@@ e@@ st\n",
      "n@@ e@@ w@@ e@@ st\n",
      "n@@ e@@ w@@ e@@ st\n",
      "n@@ e@@ w@@ e@@ st\n",
      "w@@ i@@ d@@ e@@ st\n",
      "w@@ i@@ d@@ e@@ st\n",
      "w@@ i@@ d@@ e@@ st\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "bpe = open('bpe', 'r').read()\n",
    "print(\"==segmented==\\n\" + bpe + \"====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "△ Note that only `st` is glued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3. Get vocab. \n",
    "We get vocabulary from the segmented file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocab = \"subword-nmt get-vocab < bpe > vocab\"\n",
    "os.system(get_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==vocab==\n",
      "e@@ 17\n",
      "w@@ 11\n",
      "st 9\n",
      "l@@ 7\n",
      "o@@ 7\n",
      "n@@ 6\n",
      "w 5\n",
      "i@@ 3\n",
      "d@@ 3\n",
      "r 2\n",
      "====\n",
      "number of vocab:  10\n"
     ]
    }
   ],
   "source": [
    "vocab = open('vocab', 'r').read()\n",
    "print(\"==vocab==\\n\" + vocab + \"====\")\n",
    "print(\"number of vocab: \", len(vocab.splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "△ Note that # codes (=1) is not the same as # vocab (=10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we increase the number of operations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==codes==\n",
      "#version: 0.2\n",
      "s t</w>\n",
      "e st</w>\n",
      "l o\n",
      "w est</w>\n",
      "n e\n",
      "ne west</w>\n",
      "lo w</w>\n",
      "w i\n",
      "wi d\n",
      "wid est</w>\n",
      "====\n",
      "number of codes:  10\n",
      "\n",
      "==segmented==\n",
      "low\n",
      "low\n",
      "low\n",
      "low\n",
      "low\n",
      "lo@@ w@@ e@@ r\n",
      "lo@@ w@@ e@@ r\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "widest\n",
      "widest\n",
      "widest\n",
      "====\n",
      "\n",
      "==vocab==\n",
      "newest 6\n",
      "low 5\n",
      "widest 3\n",
      "lo@@ 2\n",
      "w@@ 2\n",
      "e@@ 2\n",
      "r 2\n",
      "====\n",
      "number of vocab:  7\n"
     ]
    }
   ],
   "source": [
    "learn_bpe = \"subword-nmt learn-bpe -s 10 --min-frequency 2 < toy > codes\"\n",
    "os.system(learn_bpe)\n",
    "codes = open('codes', 'r').read()\n",
    "print(\"==codes==\\n\" + codes + \"====\")\n",
    "print(\"number of codes: \", len(codes.splitlines())-1)\n",
    "\n",
    "apply_bpe = \"subword-nmt apply-bpe -c codes < toy > bpe\"\n",
    "os.system(apply_bpe)\n",
    "bpe = open('bpe', 'r').read()\n",
    "print(\"\\n==segmented==\\n\" + bpe + \"====\")\n",
    "\n",
    "get_vocab = \"subword-nmt get-vocab < bpe > vocab\"\n",
    "os.system(get_vocab)\n",
    "vocab = open('vocab', 'r').read()\n",
    "print(\"\\n==vocab==\\n\" + vocab + \"====\")\n",
    "print(\"number of vocab: \", len(vocab.splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "△ As you've seen, if you increase the number of operations,   \n",
    "words should be less segmented,\n",
    "and the number of vocabulary should decrease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to restore the original text from the segmented one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "low\n",
      "low\n",
      "low\n",
      "low\n",
      "lower\n",
      "lower\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "widest\n",
      "widest\n",
      "widest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restored = re.sub(\"@@( |$)\", \"\", bpe)\n",
    "print(restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to restrict vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reapply_bpe = \"subword-nmt apply-bpe -c codes --vocabulary vocab --vocabulary-threshold 5 < toy > bpe2\"\n",
    "os.system(reapply_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "low\n",
      "low\n",
      "low\n",
      "low\n",
      "l@@ o@@ w@@ e@@ r\n",
      "l@@ o@@ w@@ e@@ r\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "w@@ i@@ d@@ e@@ s@@ t\n",
      "w@@ i@@ d@@ e@@ s@@ t\n",
      "w@@ i@@ d@@ e@@ s@@ t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpe2 = open('bpe2', 'r').read()\n",
    "print(bpe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "low\n",
      "low\n",
      "low\n",
      "low\n",
      "lo@@ w@@ e@@ r\n",
      "lo@@ w@@ e@@ r\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "newest\n",
      "widest\n",
      "widest\n",
      "widest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To compare with the original bpe segmented result, print it again.\n",
    "print(bpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "△ `widest`, which was not segmented, is segmented into `w@@ i@@ d@@ e@@ s@@ t` because the frequency of `widest` was less than 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful that the original vocabulary or thresholded one doesn't hold any more. We need to get the final vocabulary now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocab = \"subword-nmt get-vocab < bpe2 > vocab2\"\n",
    "os.system(get_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newest 6\n",
      "low 5\n",
      "w@@ 5\n",
      "e@@ 5\n",
      "i@@ 3\n",
      "d@@ 3\n",
      "s@@ 3\n",
      "t 3\n",
      "l@@ 2\n",
      "o@@ 2\n",
      "r 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab2 = open(\"vocab2\", 'r').read()\n",
    "print(vocab2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test with a bigger text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a sample file for demonstration from subword-nmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download = \"wget https://github.com/rsennrich/subword-nmt/raw/master/subword_nmt/tests/data/corpus.en\"\n",
    "os.system(download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould .\n",
      "iron cement protects the ingot against the hot , abrasive steel casting process .\n",
      "a fire restant repair cement for fire places , ovens , open fireplaces etc .\n",
      "construction and repair of highways and ...\n",
      "an announcement must be commercial character .\n",
      "goods and services advancement through the P.O.Box system is NOT ALLOWED .\n",
      "deliveries ( spam ) and other improper information deleted .\n",
      "translator Internet is a Toolbar for MS Internet Explorer .\n",
      "it allows you to translate in real time any web pasge from one language to another .\n",
      "you only have to select languages and TI does all the work for you ! automatic dictionary updates ....\n",
      "this software is written in order to increase your English keyboard typing speed , through teaching the basics of how to put your hand on to the keyboard and give some training examples .\n",
      "each lesson teaches some extra k\n"
     ]
    }
   ],
   "source": [
    "print(open('corpus.en', 'r').read()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==codes==\n",
      "#version: 0.2\n",
      "t h\n",
      "th e</w>\n",
      "i n\n",
      "a n\n",
      "e r\n",
      "r e\n",
      "o r\n",
      "a r\n",
      "t i\n",
      "an d</w>\n",
      "o f</w>\n",
      "e n\n",
      "o u\n",
      "o n\n",
      "t o</w>\n",
      "o n</w>\n",
      "====\n",
      "number of codes:  1000\n",
      "\n",
      "==segmented==\n",
      "ir@@ on c@@ ement is a read@@ y for use pa@@ st@@ e which is la@@ id as a fil@@ let by pu@@ t@@ ty k====\n",
      "\n",
      "==vocab==\n",
      "the 1358\n",
      ", 1291\n",
      ". 968\n",
      "and 663\n",
      "of 651\n",
      "a 623\n",
      "in 506\n",
      "to 490\n",
      "is 351\n",
      "ed 279\n",
      "s@@ 258\n",
      "c@@ 254\n",
      "you 253\n",
      "for 2====\n",
      "number of vocab:  1120\n"
     ]
    }
   ],
   "source": [
    "learn_bpe = \"subword-nmt learn-bpe -s 1000 --min-frequency 2 < corpus.en > codes\"\n",
    "os.system(learn_bpe)\n",
    "codes = open('codes', 'r').read()\n",
    "print(\"==codes==\\n\" + codes[:100] + \"====\")\n",
    "print(\"number of codes: \", len(codes.splitlines())-1)\n",
    "\n",
    "apply_bpe = \"subword-nmt apply-bpe -c codes < corpus.en > bpe\"\n",
    "os.system(apply_bpe)\n",
    "bpe = open('bpe', 'r').read()\n",
    "print(\"\\n==segmented==\\n\" + bpe[:100] + \"====\")\n",
    "\n",
    "get_vocab = \"subword-nmt get-vocab < bpe > vocab\"\n",
    "os.system(get_vocab)\n",
    "vocab = open('vocab', 'r').read()\n",
    "print(\"\\n==vocab==\\n\" + vocab[:100] + \"====\")\n",
    "print(\"number of vocab: \", len(vocab.splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (BPE in) SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"pip install sentencepiece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1. Train.  \n",
    "This should generate `m.model` and `m.vocab`. This is analogous to the `learn bpe` in `subword-nmt`. However, unlike `subword-nmt`, vocabulary, not merge operations, is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = '--input=corpus.en --model_prefix=m --vocab_size=1000 --model_type=bpe'\n",
    "spm.SentencePieceTrainer.Train(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the vocab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==vocab==\n",
      "<unk>\t0\n",
      "<s>\t0\n",
      "</s>\t0\n",
      "▁t\t-0\n",
      "▁a\t-1\n",
      "▁th\t-2\n",
      "in\t-3\n",
      "▁the\t-4\n",
      "er\t-5\n",
      "▁o\t-6\n",
      "re\t-7\n",
      "▁,\t-8\n",
      "▁s\t-9\n",
      "at\t-10\n",
      "nd\t-11\n",
      "▁.\n",
      "====\n",
      "number of vocab:  1000\n"
     ]
    }
   ],
   "source": [
    "vocab = open('m.vocab', 'r').read()\n",
    "print(\"\\n==vocab==\\n\" + vocab[:100] + \"\\n====\")\n",
    "print(\"number of vocab: \", len(vocab.splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "△ ▁, which means a space, precedes other characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2. Encode. \n",
    "First load the trained model and segment the designated text file so that all the pieces in the vocabulary should be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ ir on ▁c e ment ▁is ▁a ▁read y ▁for ▁use ▁p ast e ▁which ▁is ▁la id ▁as ▁a ▁f ill et ▁by ▁p ut t y ▁kn ife ▁or ▁f ing er ▁in ▁the ▁m ould ▁ ed g es ▁( ▁cor n ers ▁) ▁of ▁the ▁st e el ▁in g ot ▁m ould ▁. ▁ ir on ▁c e ment ▁pr ot ect s ▁the ▁in g ot ▁ag ain st ▁the ▁hot ▁, ▁ab r as ive ▁st e el ▁c ast ing ▁process ▁. ▁a ▁f ire ▁rest ant ▁rep a ir ▁c\n",
      "[923, 92, 20, 18, 924, 115, 55, 4, 596, 940, 59, 362, 28, 202, 924, 173, 55, 431, 112, 97, 4, 22, 126, 58, 158, 28, 61, 925, 940, 353, 654, 119, 22, 31, 8, 30, 7, 33, 204, 923, 27, 941, 26, 146, 888, 929, 102, 150, 29, 7, 124, 924, 60, 30, 941, 46, 33, 204, 15, 923, 92, 20, 18, 924, 115, 94, 46, 186, 930, 7, 30, 941, 46, 586, 138, 73, 7, 834, 11, 279, 931, 47, 196, 124, 924, 60, 18, 202, 31, 813, 15, 4, 22, 441, 621, 192, 449, 927, 92, 18]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"m.model\")\n",
    "\n",
    "# Segment\n",
    "input_text = open('corpus.en', 'r').read()\n",
    "pieces = sp.EncodeAsPieces(input_text)\n",
    "ids = sp.EncodeAsIds(input_text)\n",
    "print(\" \".join(pieces[:100]))\n",
    "print(ids[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to restore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould . iron cement protects the ingot against the hot , abrasive steel casting process . a fire restant repair c'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodePieces(pieces[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould . iron cement protects the ingot against the hot , abrasive steel casting process . a fire restant repair c'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds(ids[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"pip install pytorch_pretrained_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = open(\"corpus.en\", \"r\").read()\n",
    "pieces = tokenizer.tokenize(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iron cement is a ready for use paste which is laid as a fill ##et by put ##ty knife or'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(pieces[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert Tokenizer is composed of Basic Tokenizer, which splits punctuations, and WordPiece Tokenizer. That can be a problem if you want to restore the original text. https://github.com/huggingface/pytorch-pretrained-BERT/issues/36\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert Tokenizer uses ##. It is different from @@ in BPE or ▁ in SentencePiece.  \n",
    "@@ is attached to the end of subwords, while ## and ▁ is to the front.  \n",
    "`@@ + space` and `space + ##` are removed for restoration, while ▁ is replaced by a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
