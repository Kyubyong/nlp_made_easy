{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write a simple template for seq2seq using Tensorflow. For demonstration, we attack the g2p task. G2p is a task of converting graphemes (spelling) to phonemes (pronunciation). It's a very good source for this purpose as it's simple enough for you to up and run. If you want to know more about g2p, see my [repo](https://github.com/kyubyong/g2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"kyubyong\"\n",
    "__address__ = \"https://github.com/kyubyong/nlp_made_easy\"\n",
    "__email__ = \"kbpark.linguist@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEXqpZ_U738q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from distance import levenshtein\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_7vuctbU7381",
    "outputId": "f8ee2cbf-1f04-432f-ba42-d25fec61669b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6te4HKk738_"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWS2hkce739C"
   },
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params[\"batch_size\"] = 128\n",
    "params[\"test_batch_size\"] = 128\n",
    "params[\"enc_maxlen\"] = 20\n",
    "params[\"dec_maxlen\"] = 20\n",
    "params[\"num_epochs\"] = 10\n",
    "params[\"hidden_units\"] = 128\n",
    "params[\"graphemes\"] = [\"<PAD>\", \"<UNK>\", \"<EOS>\"] + list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "params[\"phonemes\"] = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"] + ['AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0',\n",
    "                'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH',\n",
    "                'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1', 'EY2', 'F', 'G', 'HH',\n",
    "                'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW0', 'OW1',\n",
    "                'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH0', 'UH1', 'UH2', 'UW',\n",
    "                'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH']\n",
    "params[\"lr\"] = 0.001\n",
    "params[\"eval_steps\"] = 500\n",
    "params[\"logdir\"] = \"logdir1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nz-hD6dn739L"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-as4PHs-739N"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('cmudict') <- if you haven't downloaded, do this.\n",
    "from nltk.corpus import cmudict\n",
    "cmu = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39gQ3vOi739S"
   },
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    g2idx = {g: idx for idx, g in enumerate(params[\"graphemes\"])}\n",
    "    idx2g = {idx: g for idx, g in enumerate(params[\"graphemes\"])}\n",
    "\n",
    "    p2idx = {p: idx for idx, p in enumerate(params[\"phonemes\"])}\n",
    "    idx2p = {idx: p for idx, p in enumerate(params[\"phonemes\"])}\n",
    "\n",
    "    return g2idx, idx2g, p2idx, idx2p # note that g and p mean grapheme and phoneme, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zslytxn6739Z"
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    words = [\" \".join(list(word)) for word, prons in cmu.items()]\n",
    "    prons = [\" \".join(prons[0]) for word, prons in cmu.items()]\n",
    "    indices = list(range(len(words)))\n",
    "    from random import shuffle\n",
    "    shuffle(indices)\n",
    "    words = [words[idx] for idx in indices]\n",
    "    prons = [prons[idx] for idx in indices]\n",
    "    num_train, num_test = int(len(words)*.8), int(len(words)*.1)\n",
    "    train_words, eval_words, test_words = words[:num_train], \\\n",
    "                                          words[num_train:-num_test],\\\n",
    "                                          words[-num_test:]\n",
    "    train_prons, eval_prons, test_prons = prons[:num_train], \\\n",
    "                                          prons[num_train:-num_test],\\\n",
    "                                          prons[-num_test:]    \n",
    "    return train_words, eval_words, test_words, train_prons, eval_prons, test_prons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHBXkAPG739j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s e l f - g o v e r n m e n t\n",
      "S EH1 L F G AH1 V ER0 N M AH0 N T\n"
     ]
    }
   ],
   "source": [
    "train_words, eval_words, test_words, train_prons, eval_prons, test_prons = prepare_data()\n",
    "print(train_words[0])\n",
    "print(train_prons[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_lengthy_samples(words, prons, enc_maxlen, dec_maxlen):\n",
    "    \"\"\"We only include such samples less than maxlen.\"\"\"\n",
    "    _words, _prons = [], []\n",
    "    for w, p in zip(words, prons):\n",
    "        if len(w.split()) + 1 > enc_maxlen: continue\n",
    "        if len(p.split()) + 1 > dec_maxlen: continue # 1: <EOS>\n",
    "        _words.append(w)\n",
    "        _prons.append(p)\n",
    "    return _words, _prons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_prons = drop_lengthy_samples(train_words, train_prons, params[\"enc_maxlen\"], params[\"dec_maxlen\"])\n",
    "eval_words, eval_prons = drop_lengthy_samples(eval_words, eval_prons, params[\"enc_maxlen\"], params[\"dec_maxlen\"])\n",
    "test_words, test_prons = drop_lengthy_samples(test_words, test_prons, params[\"enc_maxlen\"], params[\"dec_maxlen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfHMTzeH7394"
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6jSAgus7399"
   },
   "outputs": [],
   "source": [
    "def generator_fn(words, prons, padding=False, enc_maxlen=0, dec_maxlen=0):\n",
    "    '''\n",
    "    words: 1d byte array (when training) or list (when predicting). words. e.g., [b\"w o r d\", ]\n",
    "    prons: 1d byte array (when training) or list (when predicting). prons. e.g., [b'W ER1 D', ]\n",
    "    padding: boolean. If True, zeros's are padded such that the length becomes the maxlen.\n",
    "    enc_maxlen: If padding is True, this must be not 0.\n",
    "    dec_maxlen: If padding is True, this must be not 0.\n",
    "    \n",
    "    yields\n",
    "    xs: tuple of\n",
    "        x: list of encoded x. encoder input\n",
    "        x_seqlen: scalar.\n",
    "        word: string\n",
    "        \n",
    "    ys: tuple of\n",
    "        decoder_input: list of decoder inputs\n",
    "        y: list of encoded y. label.\n",
    "        y_seqlen: scalar.\n",
    "        pron: string\n",
    "    '''\n",
    "    g2idx, idx2g, p2idx, idx2p = load_vocab()\n",
    "    for word, pron in zip(words, prons):\n",
    "        w_str = word.decode(\"utf-8\") if isinstance(word, (bytes)) else word\n",
    "        p_str = pron.decode(\"utf-8\") if isinstance(pron, (bytes)) else pron\n",
    "        graphemes = w_str.split() + [\"<EOS>\"]\n",
    "        phonemes = [\"<BOS>\"] + p_str.split() + [\"<EOS>\"]\n",
    "\n",
    "        x = [g2idx.get(g, g2idx[\"<UNK>\"]) for g in graphemes]\n",
    "        y = [p2idx.get(p, p2idx[\"<UNK>\"]) for p in phonemes]\n",
    "        decoder_input, y = y[:-1], y[1:]\n",
    "        \n",
    "        x_seqlen, y_seqlen = len(x), len(y)\n",
    "        if padding:\n",
    "            x += [g2idx[\"<PAD>\"]]*(enc_maxlen - len(x))\n",
    "            decoder_input += [p2idx[\"<PAD>\"]]*(dec_maxlen - len(decoder_input))\n",
    "            y += [p2idx[\"<PAD>\"]]*(dec_maxlen - len(y))\n",
    "\n",
    "        yield (x, x_seqlen, word), (decoder_input, y, y_seqlen, pron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QEvz4YTR73-I"
   },
   "outputs": [],
   "source": [
    "def input_fn(words, prons, batch_size, shuffle=False):\n",
    "    '''Batchify data\n",
    "    words: list of words. e.g., [\"word\", ]\n",
    "    prons: list of prons. e.g., ['W ER1 D',]\n",
    "    batch_size: scalar.\n",
    "    shuffle: boolean\n",
    "    '''\n",
    "    shapes = ( ([None], (), ()),\n",
    "               ([None], [None], (), ())  )\n",
    "    types = (  (tf.int32, tf.int32, tf.string),\n",
    "               (tf.int32, tf.int32, tf.int32, tf.string)  )\n",
    "    paddings = (  (0, 0, ''),\n",
    "                  (0, 0, 0, '')  )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator_fn,\n",
    "        output_shapes=shapes,\n",
    "        output_types=types,\n",
    "        args=(words, prons)) # <- converted to np string arrays\n",
    "        \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(64*batch_size)\n",
    "    \n",
    "    dataset = dataset.repeat() # iterate forever\n",
    "    dataset = dataset.padded_batch(batch_size, shapes, paddings).prefetch(1)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22mif4xf73-M"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HA39FU4-73-O"
   },
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, params):\n",
    "        self.g2idx, self.idx2g, self.p2idx, self.idx2p = load_vocab()\n",
    "        self.params = params\n",
    "    \n",
    "    def encode(self, xs):\n",
    "        '''\n",
    "        xs: tupple of \n",
    "            x: (N, T). int32\n",
    "            seqlens: (N,). int32\n",
    "            words: (N,). string\n",
    "            \n",
    "        returns\n",
    "        last hidden: (N, hidden_units). float32\n",
    "        words: (N,). string\n",
    "        '''\n",
    "        with tf.variable_scope(\"encode\", reuse=tf.AUTO_REUSE):\n",
    "            x, seqlens, words = xs\n",
    "            x = tf.one_hot(x, len(self.g2idx))\n",
    "            cell = tf.contrib.rnn.GRUCell(self.params[\"hidden_units\"])\n",
    "            _, last_hidden = tf.nn.dynamic_rnn(cell, x, seqlens, dtype=tf.float32)\n",
    "            \n",
    "        return last_hidden, words\n",
    "        \n",
    "    \n",
    "    def decode(self, ys, h0=None):\n",
    "        '''\n",
    "        ys: tupple of \n",
    "            decoder_inputs: (N, T). int32\n",
    "            y: (N, T). int32\n",
    "            seqlens: (N,). int32\n",
    "            prons: (N,). string.\n",
    "        h0: initial hidden state. (N, hidden_units)\n",
    "        \n",
    "        returns\n",
    "        logits: (N, T, len(p2idx)). float32. before softmax\n",
    "        preds: (N, T). int32.\n",
    "        y: (N, T). int32. label.\n",
    "        prons: (N,). string. ground truth phonemes \n",
    "        last_hidden: (N, hidden_units). This is for autoregressive inference\n",
    "        '''\n",
    "        decoder_inputs, y, seqlens, prons = ys\n",
    "       \n",
    "        with tf.variable_scope(\"decode\", reuse=tf.AUTO_REUSE):\n",
    "            inputs = tf.one_hot(decoder_inputs, len(self.p2idx))\n",
    "            \n",
    "            cell = tf.contrib.rnn.GRUCell(self.params[\"hidden_units\"])\n",
    "            outputs, last_hidden = tf.nn.dynamic_rnn(cell, inputs, initial_state=h0, dtype=tf.float32)\n",
    "\n",
    "            # projection\n",
    "            logits = tf.layers.dense(outputs, len(self.p2idx))\n",
    "            preds = tf.to_int32(tf.argmax(logits, axis=-1))\n",
    "        \n",
    "        return logits, preds, y, prons, last_hidden\n",
    "            \n",
    "    def train(self, xs, ys):\n",
    "        # forward\n",
    "        last_hidden, words = self.encode(xs)\n",
    "        logits, preds, y, prons, last_hidden = self.decode(ys, h0=last_hidden)\n",
    "        \n",
    "        # train scheme\n",
    "        ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        nonpadding = tf.to_float(tf.not_equal(y, self.p2idx[\"<PAD>\"])) # 0: <pad>\n",
    "        loss = tf.reduce_sum(ce*nonpadding) / (tf.reduce_sum(nonpadding)+1e-7)\n",
    "\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(params[\"lr\"]).minimize(loss, global_step=global_step)\n",
    "        \n",
    "        return words, preds, y, prons, loss, train_op, global_step\n",
    "\n",
    "    \n",
    "    def infer(self):\n",
    "        # inputs\n",
    "        self.x = tf.placeholder(tf.int32, (None, None))\n",
    "        self.x_seqlens = tf.placeholder(tf.int32, (None,))\n",
    "        self.words = tf.placeholder(tf.string, (None,))\n",
    "\n",
    "        decoder_inputs = tf.ones((tf.shape(self.x)[0], 1), tf.int32)*self.p2idx[\"<BOS>\"]\n",
    "        \n",
    "        xs = (self.x, self.x_seqlens, self.words)\n",
    "        ys = (decoder_inputs, None, None, None)\n",
    "        \n",
    "        last_hidden, words = self.encode(xs)\n",
    "        h0 = last_hidden\n",
    "        Preds = []\n",
    "        for t in range(self.params[\"dec_maxlen\"]):\n",
    "            _, preds, _, _, h0 = self.decode(ys, h0)\n",
    "            if tf.reduce_sum(preds, 1)==0: break\n",
    "           \n",
    "            ys = (preds, None, None, None)\n",
    "            Preds.append(tf.squeeze(preds))\n",
    "        Preds = tf.stack(Preds, 1)\n",
    "        return Preds\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKllLnfp73-V"
   },
   "source": [
    "# Train & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_num_batches(total_num, batch_size):\n",
    "    return total_num // batch_size + int(total_num % batch_size != 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metric\n",
    "def per(hyp, ref):\n",
    "    '''Calc phoneme error rate\n",
    "    hyp: list of predicted phoneme sequences. e.g., [[\"B\", \"L\", \"AA1\", \"K\", \"HH\", \"AW2\", \"S\"], ...]\n",
    "    ref: list of ground truth phoneme sequences. e.g., [[\"B\", \"L\", \"AA1\", \"K\", \"HH\", \"AW2\", \"S\"], ...]\n",
    "    '''\n",
    "    num_phonemes, num_erros = 0, 0\n",
    "    for h, r in zip(hyp, ref):\n",
    "        num_phonemes += len(r)\n",
    "        num_erros += levenshtein(h, r)\n",
    "#         print(h, r, levenshtein(h, r), len(r))\n",
    "    per = round(num_erros / num_phonemes, 2)\n",
    "    return per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "64f3a-fb73-Y",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepare batches\n",
    "train_batches = input_fn(train_words, train_prons,\n",
    "                         params[\"batch_size\"], shuffle=True)\n",
    "num_train_batches = calc_num_batches(len(train_words), params[\"batch_size\"])\n",
    "\n",
    "eval_batches = input_fn(eval_words, eval_prons,\n",
    "                        params[\"batch_size\"], shuffle=False)\n",
    "num_eval_batches = calc_num_batches(len(eval_words), params[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iL3CK4NW73-g"
   },
   "outputs": [],
   "source": [
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_batches.output_types, eval_batches.output_shapes)\n",
    "xs, ys = iter.get_next()\n",
    "\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_batches)\n",
    "eval_init_op = iter.make_initializer(eval_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 14303
    },
    "colab_type": "code",
    "id": "frKAWTc873-q",
    "outputId": "2d464429-3e88-4f3f-9a9d-d64094d995f9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch\n",
      "========== epoch= 1 global step= 0 ==========\n",
      "train loss= 4.29 | eval error rate=0.99\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: F ZH N N ZH R AE1 AE1 AE1 AE1 AE1 AE1 AE1 AE1 AE1 AE1 AE1 AE1\n",
      "\n",
      "========== epoch= 1 global step= 500 ==========\n",
      "train loss= 1.88 | eval error rate=0.74\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: K K AH0 AH0 K <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "========== epoch= 2 global step= 1000 ==========\n",
      "train loss= 1.27 | eval error rate=0.66\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: P P B AH0 T <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "========== epoch= 2 global step= 1500 ==========\n",
      "train loss= 0.98 | eval error rate=0.62\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: P P B AH0 T <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> UW UW UW\n",
      "\n",
      "========== epoch= 3 global step= 2000 ==========\n",
      "train loss= 0.80 | eval error rate=0.60\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH0 P B EY2 T <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> Z Z\n",
      "\n",
      "========== epoch= 4 global step= 2500 ==========\n",
      "train loss= 0.72 | eval error rate=0.59\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH0 P B EY2 T <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> T T T T\n",
      "\n",
      "========== epoch= 4 global step= 3000 ==========\n",
      "train loss= 0.63 | eval error rate=0.58\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH0 P B EY2 T <EOS> <EOS> <EOS> <EOS> <EOS> T T T T T T T T\n",
      "\n",
      "========== epoch= 5 global step= 3500 ==========\n",
      "train loss= 0.56 | eval error rate=0.57\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH0 P B AH0 T <EOS> <EOS> <EOS> T T T T T T T T T T\n",
      "\n",
      "========== epoch= 6 global step= 4000 ==========\n",
      "train loss= 0.49 | eval error rate=0.56\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH0 P B EY2 T <EOS> <EOS> <EOS> T T T T T T T T T T\n",
      "\n",
      "========== epoch= 6 global step= 4500 ==========\n",
      "train loss= 0.44 | eval error rate=0.55\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH1 P B EY2 T <EOS> <EOS> <EOS> <EOS> T T T T T T T T T\n",
      "\n",
      "========== epoch= 7 global step= 5000 ==========\n",
      "train loss= 0.40 | eval error rate=0.54\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH1 P B EY2 T <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> T T T T T T T\n",
      "\n",
      "========== epoch= 8 global step= 5500 ==========\n",
      "train loss= 0.38 | eval error rate=0.54\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH1 P B IY2 T <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> T T T T T T T\n",
      "\n",
      "========== epoch= 8 global step= 6000 ==========\n",
      "train loss= 0.32 | eval error rate=0.53\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH1 P B IY2 T <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> T T T T T T\n",
      "\n",
      "========== epoch= 9 global step= 6500 ==========\n",
      "train loss= 0.28 | eval error rate=0.53\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH1 P B IY2 T <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> T T T T\n",
      "\n",
      "========== epoch= 10 global step= 7000 ==========\n",
      "train loss= 0.25 | eval error rate=0.53\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH1 P B IY2 T <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> T T T T\n",
      "\n",
      "========== epoch= 10 global step= 7500 ==========\n",
      "train loss= 0.25 | eval error rate=0.53\n",
      "wrd: u p b e a t\n",
      "exp: AH1 P B IY2 T\n",
      "got: AH1 P B IY2 T <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> Z Z Z Z\n",
      "\n",
      "Training Done!\n"
     ]
    }
   ],
   "source": [
    "# Training Session\n",
    "net = Net(params)\n",
    "words, preds, y, prons, loss, train_op, global_step = net.train(xs, ys)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    if tf.train.checkpoint_exists(params[\"logdir\"]):\n",
    "        ckpt = tf.train.latest_checkpoint(params[\"logdir\"])\n",
    "        print(\"Restoring from file: \", ckpt)\n",
    "        saver.restore(sess, ckpt)\n",
    "    else:\n",
    "        print(\"Initializing from scratch\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    sess.run(train_init_op)\n",
    "    for epoch in range(1, params[\"num_epochs\"]+1):\n",
    "        for _ in range(num_train_batches):\n",
    "            # training\n",
    "            _, _gs = sess.run([train_op, global_step])  \n",
    "            \n",
    "            # regular evaluation\n",
    "            if _gs%params[\"eval_steps\"]==0:\n",
    "                _loss = sess.run(loss)\n",
    "\n",
    "                sess.run(eval_init_op)\n",
    "                hyp, ref = [], []                \n",
    "                for _ in range(num_eval_batches):\n",
    "                    _words, _preds, _y, _prons = sess.run([words, preds, y, prons]) \n",
    "                    hyp.extend(_preds.tolist())\n",
    "                    ref.extend(_y.tolist())\n",
    "                    \n",
    "                ## logging\n",
    "                _word = _words[0].decode('utf-8')\n",
    "                _pron = _prons[0].decode('utf-8')\n",
    "                _pred = \" \".join(net.idx2p[each] for each in _preds[0])#.split(\"<EOS>\")[0]\n",
    "                \n",
    "                _per = per(hyp, ref)\n",
    "                \n",
    "                print(\"=\"*10, \"epoch=\", epoch, \"global step=\", _gs, \"=\"*10)\n",
    "                print(\"train loss= %.2f | eval error rate=%.2f\" % (_loss, _per))\n",
    "                print(\"wrd:\", _word)\n",
    "                print(\"exp:\", _pron)\n",
    "                print(\"got:\", _pred)\n",
    "                print()\n",
    "                \n",
    "                # save\n",
    "                if not os.path.exists(params[\"logdir\"]): os.mkdir(params[\"logdir\"])\n",
    "                fname = os.path.join(params[\"logdir\"], \"my_model_loss_%.2f_per_%.2f\" % (_loss, _per))\n",
    "                saver.save(sess, fname, global_step=_gs)\n",
    "    print(\"Training Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82t4Dmwp73--"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from logdir1/my_model_loss_0.25_per_0.53-7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint restored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:02<00:00, 33.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per: 0.32\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# prepare batches\n",
    "test_batches = list(generator_fn(test_words, test_prons, True, params[\"enc_maxlen\"], params[\"dec_maxlen\"]))\n",
    "num_test_batches = calc_num_batches(len(test_words), params[\"test_batch_size\"])\n",
    "\n",
    "net = Net(params)\n",
    "preds = net.infer()\n",
    "\n",
    "# saver for restoration\n",
    "# saver = tf.train.import_meta_graph(mname + \".meta\") <- Do NOT use this as we'll use a distinct graph.\n",
    "saver = tf.train.Saver()\n",
    "   \n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.latest_checkpoint(params[\"logdir\"])\n",
    "    saver.restore(sess, ckpt); print(\"checkpoint restored\") \n",
    "    \n",
    "    hyp = []  \n",
    "    for i in tqdm(range(num_test_batches)):\n",
    "        batch = test_batches[i*params[\"test_batch_size\"] : (i+1)*params[\"test_batch_size\"]]\n",
    "        x = [xs[0] for xs, _ in batch]\n",
    "        x_seqlens = [xs[1] for xs, _ in batch]\n",
    "        words = [xs[2] for xs, _ in batch]\n",
    "        \n",
    "        feed_dict = {net.x: x, \n",
    "                     net.x_seqlens: x_seqlens,\n",
    "                     net.words: words}\n",
    "        _preds = sess.run(preds, feed_dict)\n",
    "        hyp.extend(_preds.tolist())\n",
    "    \n",
    "    ## evaluation\n",
    "    _hyp = []\n",
    "    for phonemes in hyp:\n",
    "        each = []\n",
    "        for idx in phonemes:\n",
    "            if idx == net.p2idx[\"<EOS>\"]: break\n",
    "            each.append(net.idx2p[idx])\n",
    "        _hyp.append(each)\n",
    "    \n",
    "    ref = [pron.split() for pron in test_prons]\n",
    "    _per = per(_hyp, ref)\n",
    "    \n",
    "    # save\n",
    "    with open(\"result\", 'w') as fout:\n",
    "        fout.write(\"per: %.2f\\n\" % _per)\n",
    "        for w, r, h in zip(test_words, ref, _hyp):\n",
    "            w = w.replace(\" \", \"\")\n",
    "            r = \" \".join(r)\n",
    "            h = \" \".join(h)\n",
    "            fout.write(\"wrd: {}\\nexp: {}\\ngot: {}\\n\\n\".format(w, r, h))\n",
    "            \n",
    "    print(\"per:\", _per)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wrd: pledges',\n",
       " 'exp: P L EH1 JH IH0 Z',\n",
       " 'got: P L EH1 JH IH0 Z',\n",
       " '',\n",
       " 'wrd: combe',\n",
       " 'exp: K OW1 M',\n",
       " 'got: K AA1 M B',\n",
       " '',\n",
       " 'wrd: suspicions',\n",
       " 'exp: S AH0 S P IH1 SH AH0 N Z',\n",
       " 'got: S AH0 S P IY0 OW1 N IH0 S',\n",
       " '',\n",
       " \"wrd: fargo's\",\n",
       " 'exp: F AA1 R G OW2 Z',\n",
       " 'got: F AA1 R G OW0 Z',\n",
       " '',\n",
       " 'wrd: fizzles',\n",
       " 'exp: F IH1 Z AH0 L Z',\n",
       " 'got: F EH1 Z AH0 L Z',\n",
       " '',\n",
       " 'wrd: halon',\n",
       " 'exp: HH EY1 L AA2 N',\n",
       " 'got: HH AE1 L AH0 N',\n",
       " '',\n",
       " 'wrd: snydergeneral',\n",
       " 'exp: S N AY2 D ER0 JH EH1 N ER0 AH0 L',\n",
       " 'got: S N D EH1 G R AH0 N M AY0 L IY0',\n",
       " '',\n",
       " 'wrd: ero',\n",
       " 'exp: IH1 R OW0',\n",
       " 'got: EH1 R OW0',\n",
       " '',\n",
       " 'wrd: brockett',\n",
       " 'exp: B R AA1 K IH0 T',\n",
       " 'got: B R AA1 K IH0 T',\n",
       " '',\n",
       " 'wrd: sirna',\n",
       " 'exp: S ER1 N AH0',\n",
       " 'got: S ER1 N AH0',\n",
       " '',\n",
       " 'wrd: reuss',\n",
       " 'exp: R UW1 S',\n",
       " 'got: R UW1 S',\n",
       " '',\n",
       " 'wrd: saint',\n",
       " 'exp: S EY1 N T',\n",
       " 'got: S AE1 N T',\n",
       " '',\n",
       " 'wrd: natividad',\n",
       " 'exp: N AH0 T IH0 V IH0 D AA1 D',\n",
       " 'got: N EY1 T V IH0 D V IH0 N D',\n",
       " '',\n",
       " 'wrd: jarman',\n",
       " 'exp: JH AA1 R M AH0 N',\n",
       " 'got: JH AA1 R M AH0 N',\n",
       " '',\n",
       " 'wrd: hippler',\n",
       " 'exp: HH IH1 P L ER0',\n",
       " 'got: HH IH1 P B ER0',\n",
       " '',\n",
       " 'wrd: matzek',\n",
       " 'exp: M AE1 T Z IH0 K',\n",
       " 'got: M AE1 T S IH0 K',\n",
       " '',\n",
       " 'wrd: tallon',\n",
       " 'exp: T AE1 L AH0 N',\n",
       " 'got: T AE1 L L AH0 N',\n",
       " '',\n",
       " 'wrd: haq',\n",
       " 'exp: HH AE1 K',\n",
       " 'got: HH AE1 K',\n",
       " '',\n",
       " 'wrd: quote',\n",
       " 'exp: K W OW1 T',\n",
       " 'got: K W OW1 T',\n",
       " '',\n",
       " 'wrd: bailor',\n",
       " 'exp: B EY1 L ER0',\n",
       " 'got: B EY1 L ER0',\n",
       " '',\n",
       " 'wrd: massagers',\n",
       " 'exp: M AH0 S AA1 ZH ER0 Z',\n",
       " 'got: M AH0 S EY1 ZH ER0 Z',\n",
       " '',\n",
       " 'wrd: chords',\n",
       " 'exp: K AO1 R D Z',\n",
       " 'got: CH AO1 R D Z',\n",
       " '',\n",
       " 'wrd: announced',\n",
       " 'exp: AH0 N AW1 N S T',\n",
       " 'got: AH0 N AW1 N T S D',\n",
       " '',\n",
       " 'wrd: rayne',\n",
       " 'exp: R EY1 N',\n",
       " 'got: R EY1 N',\n",
       " '',\n",
       " 'wrd: checchi',\n",
       " 'exp: CH EH1 K IY0',\n",
       " 'got: CH EH1 K IY0',\n",
       " '']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('result', 'r').read().splitlines()[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Seq2seq tutorial with g2p.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
