{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write a simple template for seq2seq using PyTorch. For demonstration, we attack the g2p task. G2p is a task of converting graphemes (spelling) to phonemes (pronunciation). It's a very good source for this purpose as it's simple enough for you to up and run. If you want to know more about g2p, see my [repo](https:/github.com/kyubyong/g2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"kyubyong\"\n",
    "__address__ = \"https://github.com/kyubyong/nlp_made_easy\"\n",
    "__email__ = \"kbpark.linguist@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEXqpZ_U738q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from distance import levenshtein\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_7vuctbU7381",
    "outputId": "f8ee2cbf-1f04-432f-ba42-d25fec61669b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6te4HKk738_"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWS2hkce739C"
   },
   "outputs": [],
   "source": [
    "class Hparams:\n",
    "    batch_size = 128\n",
    "    enc_maxlen = 20\n",
    "    dec_maxlen = 20\n",
    "    num_epochs = 10\n",
    "    hidden_units = 128\n",
    "    emb_units = 64\n",
    "    graphemes = [\"<pad>\", \"<unk>\", \"</s>\"] + list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "    phonemes = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"] + ['AA0', 'AA1', 'AA2', 'AE0', 'AE1', 'AE2', 'AH0', 'AH1', 'AH2', 'AO0',\n",
    "                    'AO1', 'AO2', 'AW0', 'AW1', 'AW2', 'AY0', 'AY1', 'AY2', 'B', 'CH', 'D', 'DH',\n",
    "                    'EH0', 'EH1', 'EH2', 'ER0', 'ER1', 'ER2', 'EY0', 'EY1', 'EY2', 'F', 'G', 'HH',\n",
    "                    'IH0', 'IH1', 'IH2', 'IY0', 'IY1', 'IY2', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW0', 'OW1',\n",
    "                    'OW2', 'OY0', 'OY1', 'OY2', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH0', 'UH1', 'UH2', 'UW',\n",
    "                    'UW0', 'UW1', 'UW2', 'V', 'W', 'Y', 'Z', 'ZH']\n",
    "    lr = 0.001\n",
    "    logdir = \"log/02\"\n",
    "hp = Hparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nz-hD6dn739L"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-as4PHs-739N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['R', 'AH0', 'F', 'Y', 'UW1', 'Z'],\n",
       " ['R', 'EH1', 'F', 'Y', 'UW2', 'Z'],\n",
       " ['R', 'IH0', 'F', 'Y', 'UW1', 'Z']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('cmudict')# <- if you haven't downloaded, do this.\n",
    "from nltk.corpus import cmudict\n",
    "cmu = cmudict.dict()\n",
    "cmu[\"refuse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39gQ3vOi739S"
   },
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    g2idx = {g: idx for idx, g in enumerate(hp.graphemes)}\n",
    "    idx2g = {idx: g for idx, g in enumerate(hp.graphemes)}\n",
    "\n",
    "    p2idx = {p: idx for idx, p in enumerate(hp.phonemes)}\n",
    "    idx2p = {idx: p for idx, p in enumerate(hp.phonemes)}\n",
    "\n",
    "    return g2idx, idx2g, p2idx, idx2p # note that g and p mean grapheme and phoneme, respectively.\n",
    "\n",
    "g2idx, idx2g, p2idx, idx2p = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zslytxn6739Z"
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    words = [\" \".join(list(word)) for word, prons in cmu.items()]\n",
    "    prons = [\" \".join(prons[0]) for word, prons in cmu.items()]\n",
    "    indices = list(range(len(words)))\n",
    "    from random import shuffle\n",
    "    shuffle(indices)\n",
    "    words = [words[idx] for idx in indices]\n",
    "    prons = [prons[idx] for idx in indices]\n",
    "    num_train, num_test = int(len(words)*.8), int(len(words)*.1)\n",
    "    train_words, eval_words, test_words = words[:num_train], \\\n",
    "                                          words[num_train:-num_test],\\\n",
    "                                          words[-num_test:]\n",
    "    train_prons, eval_prons, test_prons = prons[:num_train], \\\n",
    "                                          prons[num_train:-num_test],\\\n",
    "                                          prons[-num_test:]    \n",
    "    return train_words, eval_words, test_words, train_prons, eval_prons, test_prons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHBXkAPG739j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c o l a b e l l a\n",
      "K OW2 L AH0 B EH1 L AH0\n"
     ]
    }
   ],
   "source": [
    "train_words, eval_words, test_words, train_prons, eval_prons, test_prons = prepare_data()\n",
    "print(train_words[0])\n",
    "print(train_prons[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_lengthy_samples(words, prons, enc_maxlen, dec_maxlen):\n",
    "    \"\"\"We only include such samples less than maxlen.\"\"\"\n",
    "    _words, _prons = [], []\n",
    "    for w, p in zip(words, prons):\n",
    "        if len(w.split()) + 1 > enc_maxlen: continue\n",
    "        if len(p.split()) + 1 > dec_maxlen: continue # 1: <EOS>\n",
    "        _words.append(w)\n",
    "        _prons.append(p)\n",
    "    return _words, _prons          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_prons = drop_lengthy_samples(train_words, train_prons, hp.enc_maxlen, hp.dec_maxlen)\n",
    "# We do NOT apply this constraint to eval and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(inp, type, dict):\n",
    "    '''convert string into ids\n",
    "    type: \"x\" or \"y\"\n",
    "    dict: g2idx for 'x', p2idx for 'y'\n",
    "    '''\n",
    "    if type==\"x\": tokens = inp.split() + [\"</s>\"]\n",
    "    else: tokens = [\"<s>\"] + inp.split() + [\"</s>\"]\n",
    "\n",
    "    x = [dict.get(t, dict[\"<unk>\"]) for t in tokens]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G2pDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, words, prons):\n",
    "        \"\"\"\n",
    "        words: list of words. e.g., [\"w o r d\", ]\n",
    "        prons: list of prons. e.g., ['W ER1 D',]\n",
    "        \"\"\"\n",
    "        self.words = words\n",
    "        self.prons = prons\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word, pron = self.words[idx], self.prons[idx]\n",
    "        x = encode(word, \"x\", g2idx)\n",
    "        y = encode(pron, \"y\", p2idx)\n",
    "        decoder_input, y = y[:-1], y[1:]\n",
    "\n",
    "        x_seqlen, y_seqlen = len(x), len(y)\n",
    "                \n",
    "        return x, x_seqlen, word, decoder_input, y, y_seqlen, pron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(batch):\n",
    "    '''Pads zeros such that the length of all samples in a batch is the same.'''\n",
    "    f = lambda x: [sample[x] for sample in batch]\n",
    "    x_seqlens = f(1)\n",
    "    y_seqlens = f(5)\n",
    "    words = f(2)\n",
    "    prons = f(-1)\n",
    "    \n",
    "    x_maxlen = np.array(x_seqlens).max()\n",
    "    y_maxlen = np.array(y_seqlens).max()\n",
    "    \n",
    "    f = lambda x, maxlen, batch: [sample[x]+[0]*(maxlen-len(sample[x])) for sample in batch]\n",
    "    x = f(0, x_maxlen, batch)\n",
    "    decoder_inputs = f(3, y_maxlen, batch)\n",
    "    y = f(4, y_maxlen, batch)\n",
    "    \n",
    "    f = torch.LongTensor\n",
    "    return f(x), x_seqlens, words, f(decoder_inputs), f(y), y_seqlens, prons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22mif4xf73-M"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    global g2idx, idx2g, p2idx, idx2p\n",
    "    def __init__(self, emb_units, hidden_units):\n",
    "        super().__init__()\n",
    "        self.emb_units = emb_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.emb = nn.Embedding(len(g2idx), emb_units)\n",
    "        self.rnn = nn.GRU(emb_units, hidden_units, batch_first=True)\n",
    "        \n",
    "    def forward(self, x, seqlens):\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # reverse sorting by length\n",
    "        seqlens = torch.IntTensor(seqlens)\n",
    "        seqlens_sorted, perm_idx = seqlens.sort(0, descending=True)\n",
    "        _, unperm_idx = perm_idx.sort(0) # for recovery\n",
    "        x = x[perm_idx]\n",
    "            \n",
    "        # packing -> rnn -> unpacking -> position recovery\n",
    "        packed_input = pack_padded_sequence(x, seqlens_sorted, batch_first=True)   \n",
    "        outputs, last_hidden = self.rnn(packed_input)\n",
    "#         outputs, _ = pad_packed_sequence(outputs, batch_first=True, total_length=x.size()[1])\n",
    "#         outputs = outputs[unperm_idx]\n",
    "\n",
    "        # last hidden\n",
    "        last_hidden = last_hidden.permute(1, 2, 0)\n",
    "        last_hidden = last_hidden.view(last_hidden.size()[0], -1)\n",
    "        last_hidden = last_hidden[unperm_idx]\n",
    "        \n",
    "        return last_hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    global g2idx, idx2g, p2idx, idx2p\n",
    "    def __init__(self, emb_units, hidden_units):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_units = emb_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.emb = nn.Embedding(len(p2idx), emb_units)\n",
    "        self.rnn = nn.GRU(emb_units, hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, len(p2idx))\n",
    "        \n",
    "    def forward(self, decoder_inputs, h0):\n",
    "        decoder_inputs = self.emb(decoder_inputs)\n",
    "           \n",
    "        outputs, last_hidden = self.rnn(decoder_inputs, h0)\n",
    "        logits = self.fc(outputs) # (N, T, V)\n",
    "        y_hat = logits.argmax(-1)\n",
    "        \n",
    "        return logits, y_hat, last_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HA39FU4-73-O"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    global g2idx, idx2g, p2idx, idx2p\n",
    "    \n",
    "    def __init__(self, encoder, decoder): \n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x, seqlens, decoder_inputs, teacher_forcing=True, dec_maxlen=None):  \n",
    "        '''\n",
    "        At training, teacher forcing is True.\n",
    "        if teacher_forcing is True, dec_maxlen must be given.\n",
    "        '''\n",
    "        last_hidden = self.encoder(x, seqlens)\n",
    "        h0 = last_hidden.unsqueeze(0)\n",
    "        \n",
    "        if teacher_forcing:\n",
    "            logits, y_hat, h0 = self.decoder(decoder_inputs, h0)\n",
    "        else:\n",
    "            logits, y_hat = [], []\n",
    "            for t in range(dec_maxlen):\n",
    "                _logits, _y_hat, h0 =self.decoder(decoder_inputs[:, t:t+1, :], h0) # logits: (N, 1, V), y_hat: (N, 1)\n",
    "                logits.append(_logits)\n",
    "                y_hat.append(_y_hat)\n",
    "                if _y_hat.sum(1)==0: break # all samples reach padding.\n",
    "        \n",
    "            logits = torch.cat(logits, 1)\n",
    "            y_hat = torch.cat(y_hat, 1)\n",
    "        \n",
    "        return logits, y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        x, x_seqlens, words, decoder_inputs, y, y_seqlens, prons = batch\n",
    "        \n",
    "        x, decoder_inputs = x.to(device), decoder_inputs.to(device) \n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, y_hat = model(x, x_seqlens, decoder_inputs)\n",
    "        \n",
    "        # calc loss\n",
    "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
    "        y = y.view(-1) # (N*T,)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i and i%100==0:\n",
    "            print(f\"step: {i}, loss: {loss.item()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_per(Y_true, Y_pred):\n",
    "    '''Calc phoneme error rate\n",
    "    Y_true: list of predicted phoneme sequences. e.g., [[\"B\", \"L\", \"AA1\", \"K\", \"HH\", \"AW2\", \"S\"], ...]\n",
    "    Y_pred: list of ground truth phoneme sequences. e.g., [[\"B\", \"L\", \"AA1\", \"K\", \"HH\", \"AW2\", \"S\"], ...]\n",
    "    '''\n",
    "    num_phonemes, num_erros = 0, 0\n",
    "    for y_true, y_pred in zip(Y_true, Y_pred):\n",
    "        num_phonemes += len(y_true)\n",
    "        num_erros += levenshtein(y_true, y_pred)\n",
    "\n",
    "    per = round(num_erros / num_phonemes, 2)\n",
    "    return per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids_to_phonemes(ids, idx2p):\n",
    "    phonemes = []\n",
    "    for idx in ids:\n",
    "        if idx == 3: # 3: </s>\n",
    "            break\n",
    "        p = idx2p[idx]\n",
    "        phonemes.append(p)\n",
    "    return phonemes\n",
    "        \n",
    "            \n",
    "\n",
    "def eval(model, iterator, device, dec_maxlen):\n",
    "    model.eval()\n",
    "\n",
    "    Y_true, Y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            x, x_seqlens, words, decoder_inputs, y, y_seqlens, prons = batch\n",
    "            x, decoder_inputs = x.to(device), decoder_inputs.to(device) \n",
    "\n",
    "            _, y_hat = model(x, x_seqlens, decoder_inputs, dec_maxlen)\n",
    "            \n",
    "            y = y.to('cpu').numpy().tolist()\n",
    "            y_hat = y_hat.to('cpu').numpy().tolist()\n",
    "            for yy, yy_hat in zip(y, y_hat):\n",
    "                y_true = convert_ids_to_phonemes(yy, idx2p)\n",
    "                y_pred = convert_ids_to_phonemes(yy_hat, idx2p)\n",
    "                Y_true.append(y_true)\n",
    "                Y_pred.append(y_pred)\n",
    "    \n",
    "    # calc per.\n",
    "    per = calc_per(Y_true, Y_pred)\n",
    "    print(\"per: %.2f\" % per)\n",
    "    \n",
    "    with open(\"result\", \"w\") as fout:\n",
    "        for y_true, y_pred in zip(Y_true, Y_pred):\n",
    "            fout.write(\" \".join(y_true) + \"\\n\")\n",
    "            fout.write(\" \".join(y_pred) + \"\\n\\n\")\n",
    "    \n",
    "    return per\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKllLnfp73-V"
   },
   "source": [
    "# Train & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = G2pDataset(train_words, train_prons)\n",
    "eval_dataset = G2pDataset(eval_words, eval_prons)\n",
    "\n",
    "train_iter = data.DataLoader(train_dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=pad)\n",
    "eval_iter = data.DataLoader(eval_dataset, batch_size=hp.batch_size, shuffle=False, collate_fn=pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "aF0cJceg73-m",
    "outputId": "ab78b80a-d6e3-4408-af21-acff58944a79",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1\n",
      "step: 100, loss: 2.681352138519287\n",
      "step: 200, loss: 2.038400650024414\n",
      "step: 300, loss: 1.6250944137573242\n",
      "step: 400, loss: 1.4096786975860596\n",
      "step: 500, loss: 1.3086246252059937\n",
      "step: 600, loss: 1.158888578414917\n",
      "step: 700, loss: 0.9511236548423767\n",
      "per: 0.32\n",
      "\n",
      "epoch: 2\n",
      "step: 100, loss: 0.8988334536552429\n",
      "step: 200, loss: 0.8190440535545349\n",
      "step: 300, loss: 0.8058145642280579\n",
      "step: 400, loss: 0.7160618305206299\n",
      "step: 500, loss: 0.6768094301223755\n",
      "step: 600, loss: 0.748418927192688\n",
      "step: 700, loss: 0.64652019739151\n",
      "per: 0.23\n",
      "\n",
      "epoch: 3\n",
      "step: 100, loss: 0.6145666241645813\n",
      "step: 200, loss: 0.601494312286377\n",
      "step: 300, loss: 0.554980993270874\n",
      "step: 400, loss: 0.5778650045394897\n",
      "step: 500, loss: 0.5602371096611023\n",
      "step: 600, loss: 0.5774532556533813\n",
      "step: 700, loss: 0.5515686869621277\n",
      "per: 0.19\n",
      "\n",
      "epoch: 4\n",
      "step: 100, loss: 0.5521383881568909\n",
      "step: 200, loss: 0.5028484463691711\n",
      "step: 300, loss: 0.4936176836490631\n",
      "step: 400, loss: 0.4618658125400543\n",
      "step: 500, loss: 0.5209434032440186\n",
      "step: 600, loss: 0.44204363226890564\n",
      "step: 700, loss: 0.5374690890312195\n",
      "per: 0.17\n",
      "\n",
      "epoch: 5\n",
      "step: 100, loss: 0.4316357672214508\n",
      "step: 200, loss: 0.4500748813152313\n",
      "step: 300, loss: 0.42680609226226807\n",
      "step: 400, loss: 0.4144856631755829\n",
      "step: 500, loss: 0.408010333776474\n",
      "step: 600, loss: 0.4486962854862213\n",
      "step: 700, loss: 0.5073139667510986\n",
      "per: 0.17\n",
      "\n",
      "epoch: 6\n",
      "step: 100, loss: 0.44076693058013916\n",
      "step: 200, loss: 0.42301252484321594\n",
      "step: 300, loss: 0.4174518585205078\n",
      "step: 400, loss: 0.4315039813518524\n",
      "step: 500, loss: 0.41985586285591125\n",
      "step: 600, loss: 0.40766441822052\n",
      "step: 700, loss: 0.362885445356369\n",
      "per: 0.16\n",
      "\n",
      "epoch: 7\n",
      "step: 100, loss: 0.375058650970459\n",
      "step: 200, loss: 0.34482550621032715\n",
      "step: 300, loss: 0.46471866965293884\n",
      "step: 400, loss: 0.36243101954460144\n",
      "step: 500, loss: 0.38823458552360535\n",
      "step: 600, loss: 0.4026390612125397\n",
      "step: 700, loss: 0.35270220041275024\n",
      "per: 0.15\n",
      "\n",
      "epoch: 8\n",
      "step: 100, loss: 0.30141958594322205\n",
      "step: 200, loss: 0.38777515292167664\n",
      "step: 300, loss: 0.38467416167259216\n",
      "step: 400, loss: 0.37791141867637634\n",
      "step: 500, loss: 0.4574853777885437\n",
      "step: 600, loss: 0.3224585950374603\n",
      "step: 700, loss: 0.3900456726551056\n",
      "per: 0.15\n",
      "\n",
      "epoch: 9\n",
      "step: 100, loss: 0.3075313866138458\n",
      "step: 200, loss: 0.3181159198284149\n",
      "step: 300, loss: 0.33302876353263855\n",
      "step: 400, loss: 0.36537471413612366\n",
      "step: 500, loss: 0.34021782875061035\n",
      "step: 600, loss: 0.3173317611217499\n",
      "step: 700, loss: 0.381842702627182\n",
      "per: 0.14\n",
      "\n",
      "epoch: 10\n",
      "step: 100, loss: 0.3363656997680664\n",
      "step: 200, loss: 0.3434205651283264\n",
      "step: 300, loss: 0.3297593593597412\n",
      "step: 400, loss: 0.38416537642478943\n",
      "step: 500, loss: 0.3832787275314331\n",
      "step: 600, loss: 0.29887327551841736\n",
      "step: 700, loss: 0.3148936927318573\n",
      "per: 0.13\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(hp.emb_units, hp.hidden_units)\n",
    "decoder = Decoder(hp.emb_units, hp.hidden_units)\n",
    "model = Net(encoder, decoder)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = hp.lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "for epoch in range(1, hp.num_epochs+1):\n",
    "    print(f\"\\nepoch: {epoch}\")\n",
    "    train(model, train_iter, optimizer, criterion, device)\n",
    "    eval(model, eval_iter, device, hp.dec_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82t4Dmwp73--"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUyYlI4S73_O",
    "outputId": "ae0592d3-14b0-4f3b-94f8-ebc293c48304"
   },
   "outputs": [],
   "source": [
    "test_dataset = G2pDataset(test_words, test_prons)\n",
    "\n",
    "test_iter = data.DataLoader(test_dataset, batch_size=hp.batch_size, shuffle=False, collate_fn=pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per: 0.14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(model, test_iter, device, hp.dec_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'L EH1 F T W IH0 K',\n",
       " 'L EH1 F T W IH2 K',\n",
       " '',\n",
       " 'R AO1 N CH IY0',\n",
       " 'R AO1 N CH IY0',\n",
       " '',\n",
       " 'HH AH1 F AY2 N Z',\n",
       " 'HH AH1 F IY0 N Z',\n",
       " '',\n",
       " 'B AH1 T ER0 F L AY2',\n",
       " 'B AH1 T ER0 F L IY0',\n",
       " '',\n",
       " 'S AO1 D IY0 Z',\n",
       " 'S AO1 D IH0 Z',\n",
       " '',\n",
       " 'N AO2 R W EH1 S T S',\n",
       " 'N AO1 R W EH1 S T S',\n",
       " '',\n",
       " 'K AA1 K R OW2 CH',\n",
       " 'K AA1 K R OW2 K',\n",
       " '',\n",
       " 'S T AY1 N W EY2',\n",
       " 'S T AY1 N W EY2',\n",
       " '',\n",
       " 'G R IH1 L D',\n",
       " 'G R IH1 L D',\n",
       " '',\n",
       " 'K AY1 Z ER0 Z B ER0 G',\n",
       " 'K EY1 S ER0 S B ER0 G',\n",
       " '',\n",
       " 'SH R AY1 N ER0 Z',\n",
       " 'SH R AY1 N ER0 Z',\n",
       " '',\n",
       " 'K R AA1 K S T AH0 N',\n",
       " 'K R AA1 K S T AH0 N',\n",
       " '',\n",
       " 'V AH0 SH IH1 N S K IY0',\n",
       " 'V AE2 SH IH1 N S K IY0',\n",
       " '',\n",
       " 'P AE2 TH AH0 L AA1 JH IH0 K L IY0',\n",
       " 'P AE2 TH AH0 L AA1 G IH0 K L IY0',\n",
       " '',\n",
       " 'P AH0 L AA1 N D R IY0',\n",
       " 'P OW0 L AA1 N D R IY0',\n",
       " '',\n",
       " 'P IH1 K ER0 AH0 N',\n",
       " 'P IH1 K R N N',\n",
       " '',\n",
       " 'P IH1 K L IH0 NG',\n",
       " 'P IH1 K L IH0 NG',\n",
       " '',\n",
       " 'R AA1 S N ER0',\n",
       " 'R AA1 S N ER0',\n",
       " '',\n",
       " 'JH UH1 R AH0 Z',\n",
       " 'JH UH1 R AH0 Z',\n",
       " '',\n",
       " 'B UW1 N',\n",
       " 'B UW1 N',\n",
       " '',\n",
       " 'AA1 R L IH0 NG T AH0 N Z',\n",
       " 'AA1 R L IH0 NG T AH0 N Z',\n",
       " '',\n",
       " 'M ER1 V IH0 N',\n",
       " 'M ER1 V IH0 N',\n",
       " '',\n",
       " 'AE2 N T IY0 HH IH1 S T AH0 M AH0 N Z',\n",
       " 'AE2 N T IH1 AE1 IH1 S T AH0 M AH0 N Z',\n",
       " '',\n",
       " 'L AA1 NG P ER0',\n",
       " 'L AO1 NG P R G',\n",
       " '',\n",
       " 'AE1 S T',\n",
       " 'AE1 S IH0',\n",
       " '',\n",
       " 'B R AY1 N',\n",
       " 'B R AY1 N',\n",
       " '',\n",
       " 'F IH1 L IH0 P S IH0 Z',\n",
       " 'F IH1 L IH0 P S IH0 Z',\n",
       " '',\n",
       " 'K AO0 R IY1 AH0 N',\n",
       " 'K AO1 R IY1 N N',\n",
       " '',\n",
       " 'S AE1 L V AH0 D AO2 R',\n",
       " 'S AE1 L V AH0 D ER0 R',\n",
       " '',\n",
       " 'HH AE1 CH ER0',\n",
       " 'HH AE1 CH ER0',\n",
       " '',\n",
       " 'P R IY0 N AH1 P SH AH0 L',\n",
       " 'P R IH0 N AH1 P SH AH0 L',\n",
       " '',\n",
       " 'S UW1 P AH0',\n",
       " 'S UW1 P AH0',\n",
       " '',\n",
       " 'SH R EH1 K',\n",
       " 'SH R EH1 K',\n",
       " '']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('result', 'r').read().splitlines()[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Seq2seq tutorial with g2p.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
