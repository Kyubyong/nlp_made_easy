{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is arguably the most popular regularization technique in deep learning. Let's check again how it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"kyubyong\"\n",
    "__address__ = \"https://github.com/kyubyong/nlp_made_easy\"\n",
    "__email__ = \"kbpark.linguist@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, keep_prob=1.):\n",
    "        # Inputs\n",
    "        x = tf.expand_dims(tf.convert_to_tensor([1.], tf.float32), 1)\n",
    "        y = tf.expand_dims(tf.convert_to_tensor([2.], tf.float32), 1)\n",
    "\n",
    "        # Variables\n",
    "        w1 = tf.Variable([[0.1, -0.1, 0.2]], dtype=tf.float32, name=\"weight1\")\n",
    "\n",
    "        # fully connected layer (a.k.a. dense layer)\n",
    "        h = tf.nn.relu(tf.matmul(x, w1))\n",
    "        self.h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
    "\n",
    "        # Readout layer\n",
    "        w2 = tf.Variable([[0.2], [0.1], [-0.1]], dtype=tf.float32, name=\"weight2\")\n",
    "        self.pred = tf.matmul(self.h, w2)\n",
    "\n",
    "        # Loss\n",
    "        self.loss = tf.reduce_mean(tf.square(self.pred - y)) # L2 loss\n",
    "\n",
    "        # Training scheme\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "        self.grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(self.grads_and_vars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(keep_prob=1.):\n",
    "    g = Graph(keep_prob=keep_prob)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # feed-forward and back-prop for getting gradients\n",
    "        loss, hidden_units, output, _, _grads_and_vars = sess.run([g.loss, g.h, g.pred, g.train_op, g.grads_and_vars])\n",
    "        grad1 = _grads_and_vars[0][0]\n",
    "        grad2 = _grads_and_vars[1][0]\n",
    "                              \n",
    "    return loss, hidden_units, output, grad1, grad2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, hidden_units, output, grad1, grad2 = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of no dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 4.0\n",
      "hidden units= [[0.1 0.  0.2]]\n",
      "y_hat= [[0.]]\n",
      "grad1= [[-0.8  0.   0.4]]\n",
      "grad2= [[-0.4]\n",
      " [ 0. ]\n",
      " [-0.8]]\n"
     ]
    }
   ],
   "source": [
    "print(\"loss=\", loss)\n",
    "print(\"hidden units=\", hidden_units)\n",
    "print(\"y_hat=\", output)\n",
    "print(\"grad1=\", grad1)\n",
    "print(\"grad2=\", grad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"no-dropout.png\" width=500. align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients flow back through the first and third units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of dropouts (50:50 prob.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "_loss, _hidden_units, _output, _grad1, _grad2 = run(keep_prob=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 3.8416002\n",
      "hidden units= [[0.2 0.  0. ]]\n",
      "y_hat= [[0.04]]\n",
      "grad1= [[-1.57  0.    0.  ]]\n",
      "grad2= [[-0.78]\n",
      " [ 0.  ]\n",
      " [ 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"loss=\", _loss)\n",
    "print(\"hidden units=\", _hidden_units)\n",
    "print(\"y_hat=\", _output)\n",
    "print(\"grad1=\", _grad1)\n",
    "print(\"grad2=\", _grad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dropout.png\" width=500. align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The third unit becomes zero, so the gradient flows back through only the first unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
